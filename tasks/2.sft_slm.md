# Supervised Fine-tuning Small Language Models

## Model 1: Gemma-3N (4B) Conversational SFT
- Adapt the tutorial from  https://unsloth.ai/ Colab notebook [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb) or.  
- Use adapted Colab notebook: [Gemma3N_(4B)_Conversational.ipynb](./Gemma3N_(4B)_Conversational.ipynb)


## Model 2: GPT-OSS (20B) SFT
- Adapt the tutorial from  https://unsloth.ai/ Colab notebook [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb) or.  
- Use adapted Colab notebook: [gpt_oss_(20B)_Fine_tuning.ipynb](./gpt_oss_(20B)_Fine_tuning.ipynb)


## Model 3: IBM Granite-4 (8B) SFT
- Adapt the tutorial from  https://unsloth.ai/ Colab notebook [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Granite4.0.ipynb) or.  
- Use adapted Colab notebook: [Granite4_0.ipynb](./Granite4_0.ipynb)


## Convert to Ollama format

1. Gemma-3N example:
```bash
model.save_pretrained_gguf(
    "gemma-3N-finetune",
    tokenizer,
    quantization_method = "Q8_0"   # or "f16", "q4_k_m", etc.
)
```

Failai:

```
gemma-3N-finetune/unsloth.Q8_0.gguf
# ..
```

2. iname:
```bash
cd gemma-3N-finetune
```

3. Sukuriame failą `Modelfile` su turiniu:
```bash
FROM ./unsloth.Q8_0.gguf

TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""

SYSTEM """You are a helpful AI assistant."""

PARAMETER temperature 1.0
PARAMETER top_p 0.95
PARAMETER top_k 64
PARAMETER repeat_penalty 1.0
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|eot_id|>"
```

4. Konvertuojame į Ollama formatą:
```bash
ollama show hf.co/unsloth/gemma-3n-E4B-it-GGUF:Q8_0 --modelfile > Modelfile
ollama create gemma-3n-my-finetune -f Modelfile
```

5. Naudojame savo fine-tuned modelį:
```bash
ollama run gemma-3n-my-finetune
```


Return to [README](../README.md).md.